{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_json, DotaTokenizer\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "hero_dpath = os.path.join(project_root, 'data', 'heroes.json')\n",
    "match_dpath = os.path.join(project_root, 'data', 'main_metadata_2023.csv')\n",
    "pickbans_dpath = os.path.join(project_root, 'data', 'picks_bans_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks_df = pd.read_csv(pickbans_dpath)\n",
    "matches_df = pd.read_csv(match_dpath)\n",
    "hero_metadata = load_json(hero_dpath)\n",
    "\n",
    "results = np.array(matches_df)\n",
    "picks = np.array(picks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tks = ['[PAD]', '[PICK]', '[BAN]', '[RADSTART]', '[DIRESTART]', '[RADWIN]', '[DIREWIN]']\n",
    "tokenizer = DotaTokenizer(hero_metadata, special_tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "picks_df columns: is_pick, hero_id, team, order, match_id\n",
    "results_df = match_id, radiant_win\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_array(tokenizer: DotaTokenizer, picks: np.array, results: np.array, hero_metadata: dict) -> tuple[str, int]:\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(tokenizer: DotaTokenizer, picks: np.array, results: np.array, hero_metadata: dict) -> np.array:\n",
    "\n",
    "    dataset = np.array([0, 0])\n",
    "\n",
    "    # Transform into one single function\n",
    "    umps, umrs = np.unique(picks[:, 4]), np.unique(results[:, 0]) # Col 4 represents 'match_id', Col 0 representes 'match_id'\n",
    "    conflicting_matches = np.setxor1d(umps, umrs)\n",
    "    mask_picks, mask_results = ~np.isin(picks[:, 4], conflicting_matches), ~np.isin(results[:, 0], conflicting_matches)\n",
    "    picks, results = picks[mask_picks], results[mask_results]\n",
    "\n",
    "    # Get rid of matches with less than 24 picks\n",
    "\n",
    "    umps = np.unique(picks[:, 4]) # Recompute the indices since the original computation may include pruned indices\n",
    "    matches_mask = \n",
    "    \n",
    "    #samples = make_sample_array(tokenizer, picks, results, hero_metadata)\n",
    "\n",
    "    return picks, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks, results = make_dataset(tokenizer, picks, results, hero_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "negsampler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
